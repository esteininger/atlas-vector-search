{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b32573",
   "metadata": {},
   "source": [
    "## Sparse Vector Tutorial\n",
    "\n",
    "Transistors are the building blocks of computers, and they understand the world with 1's and 0's. Consequentially, in order to build systems that interpret the world, conversion into 1's and 0's needs to be applied. \n",
    "\n",
    "In this walkthrough, we'll create a [Bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model). This is a method of \"feature extraction\" from text data. Features, in this case are these numerical (1's and 0's) representations of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "import ssl\n",
    "from pprint import pp\n",
    "\n",
    "# because we don't care about security. sweat emoji.\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc028efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the entire Japan wikipedia article\n",
    "html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Japan').read()\n",
    "article = bs.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# extract all the paragraphs\n",
    "paragraphs = article.find_all('p')\n",
    "\n",
    "pp(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = ''\n",
    "\n",
    "# get just the content of the article\n",
    "for para in paragraphs:\n",
    "    article_text += para.text\n",
    "\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b7321",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "In order to find similar strings, we need to get to the \"core\" of each word. This is what tokenization does. We'll create an incredbily basic tokenizer function which:\n",
    "\n",
    "1. Splits the article by sentence\n",
    "2. Normalizes it into all lowercase \n",
    "3. Retrieves only the stems\n",
    "4. Remove all punctuation\n",
    "5. Remove all stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Stemmer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# simply breaking a string up by whitespace into an array of strings.\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# converting every string into lowercarse\n",
    "def lowercase_filter(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "# applying the stemmer library to get every word to its' root\n",
    "def stem_filter(tokens):\n",
    "    STEMMER = Stemmer.Stemmer('english')\n",
    "    return STEMMER.stemWords(tokens)\n",
    "\n",
    "# remove all punctuation\n",
    "def punctuation_filter(tokens):\n",
    "    PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    return [PUNCTUATION.sub('', token) for token in tokens]\n",
    "\n",
    "# remove all stop words\n",
    "# These are the top 25 most common words in English according to wikipedia: https://en.wikipedia.org/wiki/Most_common_words_in_English\n",
    "def stopword_filter(tokens):\n",
    "    STOPWORDS = set(['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',\n",
    "                     'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',\n",
    "                     'do', 'at', 'this', 'but', 'his', 'by', 'from', 'wikipedia'])\n",
    "    return [token for token in tokens if token not in STOPWORDS]\n",
    "\n",
    "# Now an \"analyze\" function which puts all of the functions above into one single function:\n",
    "def analyze(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = lowercase_filter(tokens)\n",
    "    tokens = punctuation_filter(tokens)\n",
    "    tokens = stopword_filter(tokens)\n",
    "    tokens = stem_filter(tokens)\n",
    "\n",
    "    return [token for token in tokens if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a97000",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = analyze(article_text)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word frequency of each word in the corpus\n",
    "\n",
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "\n",
    "print(wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9846c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['japan', 'is', 'japanes', 'has', 'world', 'are', 'nation', 'was', 'countri', 'which', 'it', 'war', 'an', 'period', 'includ', 'been', 'state', 'use', 'among', 'popul', 'one', 'island', 'largest', 'sinc', 'centuri', 'after', 'unit', 'most', 'develop', 'popular', 'or', 'power', 'militari', 'dure', 'govern', 'china', 'cultur', 'other', 'such', 'day', 'tradit', 'constitut', 'south', 'million', 'first', 'emperor', 'forc', 'industri', 'highest', 'intern', 'main', 'high', 'known', 'languag', 'system', 'major', 'tokyo', 'chines', 'earli', 'influenc', 'were', 'over', 'total', 'asia', 'well', 'region', 'begin', 'rank', 'western', 'all', 'law', 'follow', 'area', 'between', 'gdp', 'establish', 'water', 'number', 'year', 'larg', 'climat', 'secur', 'rate', 'age', 'educ', 'about', 'into', 'more', 'base', 'polit', 'adopt', 'ii', 'began', 'econom', 'becom', 'signific', 'around', 'introduc', 'local', 'school', 'mani', 'korea', 'becaus', 'winter', 'summer', 'import', 'environment', 'host', 'relat', 'public', 'sea', 'north', 'under', 'seri', 'two', 'maintain', 'global', 'member', 'oecd', 'offici', 'land', 'peopl', 'gain', 'remain', 'control', 'increas', 'earthquak', 'four', 'nuclear', 'may', '2019', 'level', 'domest', 'percent', 'sector', 'space', '1', 'east', 'pacif', 'while', 'part', 'kilomet', 'prefectur', 'becam', 'shogun', 'modern', 'alli', 'new', 'art', 'music', 'edo', 'central', 'buddhism', 'result', 'second', 'veri', 'natur', 'heavi', 'ministri', '2020', 'cooper', 'univers', 'had', 'market', 'export', 'particular', 'fish', 'competit', 'produc', 'research', 'health', 'form', 'monday', '日本', 'hokkaido', 'citi', 'bc', 'written', 'imperi', 'court', 'foreign', 'polici', 'led', 'meiji', 'empir', 'program', 'economi', 'selfdefens', 'record', 'made', 'scienc', 'game', 'name', 'chang', 'sourc', 'through', 'oldest', 'continu', 'korean', 'reform']\n"
     ]
    }
   ],
   "source": [
    "# What are the 200 most frequently used words?\n",
    "import heapq\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "\n",
    "print(most_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93d16363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Now we convert the corpus into sparse vector representations (1's and 0's)\n",
    "# this block of code finds each word in the most_freq array and checks if the word exists in the article\n",
    "\n",
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)\n",
    "\n",
    "sentence_vectors = np.asarray(sentence_vectors)\n",
    "\n",
    "print(sentence_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de010f40",
   "metadata": {},
   "source": [
    "## Future\n",
    "\n",
    "In this tutorial we walked you through the onversion of content in the form of text into its' corresponding vector representations. These are sparse vectors, meaning each token is converted into a feature which is just a single digit representation (1's and 0's). \n",
    "\n",
    "There is still the process of how to retrieve similar texts as features, and to do this your search engine would simply convert the user-supplied query into tokens, then find their most similar corresponding feature embeddings.\n",
    "\n",
    "The next step in this walkthrough will be the conversion of a similar corpus into its' dense vector representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237f145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
